{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SparkSession\n",
    "\n",
    "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
    "\n",
    "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f3cb89e5048>\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "# Libraries\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types\n",
    "# Create a Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# How is my session?\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3cb89e5048>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "path = '../Master_DataScience/06_Machine_Learning_on_my_own/Files/'\n",
    "file = 'train.csv'\n",
    "# Load\n",
    "df = spark.read.csv(path+file,header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|\n",
      "|       0|     3|  male|null|    0|    0| 8.4583|       Q|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|\n",
      "|       1|     2|  male|null|    0|    0|   13.0|       S|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|\n",
      "|       1|     3|female|null|    0|    0|  7.225|       C|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select('Survived',\n",
    "                'Pclass',\n",
    "                'Sex',\n",
    "                'Age',\n",
    "                'SibSp',\n",
    "                'Parch',\n",
    "                'Fare',\n",
    "                'Embarked')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Null Values and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
       "0         0       0    0  177      0      0     0         2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null Values\n",
    "df_agg = df2.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in df2.columns]) \n",
    "df_agg.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputations (Replace Unknowns & Missing Values) or Dropna\n",
    "\n",
    "# # Filter\n",
    "# df2 = df2.filter(condition...)\n",
    "# # Missing Value Imputation\n",
    "# df2 = df2.fillna(-1, subset=[\"Age\"])\n",
    "# df2 = df2.fillna(-1, subset=[\"Cabin\"])\n",
    "# df2 = df2.fillna(-1, subset=[\"Embarked\"])\n",
    "# # New Columns\n",
    "# df2 = df2.withColumn('Sex_cat',f.when(f.col('Sex')== 'male',f.lit(1)).otherwise(0))\n",
    "\n",
    "# This is a basic example, we will drop all our missing values\n",
    "df3 = df2.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
       "0         0       0    0    0      0      0     0         0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null Values after handling missing values\n",
    "df_agg = df3.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in df3.columns]) \n",
    "df_agg.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Handle different fields in different ways \n",
    "\n",
    "We have features of at least three kinds:\n",
    "\n",
    "* __Numeric continuous fields__, which we can use as input to many algorithms as they are. In particular, decision trees can take continuous variables with any value as input, since they only look for the cutoff point that most increases the homogeneity of the resulting groups. In contrast, if we were using a logistic regression with regularization, for example, we would need to first scale the variables to have comparable magnitudes.\n",
    "\n",
    "* There are fields which we will treat as __categorical variables__, but which are already integers. These need to be __one-hot encoded__.\n",
    "\n",
    "* Finally, there are several __categorical variables that are encoded as strings__. These need to be one-hot encoded, but __OneHotEncoder__ requires numeric input. Therefore, we will need to apply a __StringIndexer__ to each of them before one-hot encoding.\n",
    "\n",
    "___PySpark Pipeline___\n",
    "\n",
    "| Types      |StrIndex   | OneHot   | Model  |\n",
    "|----------- | --------- | -------- | ----   |\n",
    "|CAT         | NO        | YES      | YES    |\n",
    "|STR         | YES       | YES      | YES    |\n",
    "|CONT        | NO        | NO       | YES    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. StringIndexer \n",
    "\n",
    "A [StringIndexer](https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer) is an estimator that takes a single string field, then produces a transformer that codifies said field as numeric labels that are fit for feeding to a one-hot encoding. \n",
    "\n",
    "We need to specify an input column, an output column, and a way to handle invalids. In this case, invalids are values that the indexer has not seen during fitting but that the transformer finds during processing. Its values are 'error' (the default), which is pretty self-explanatory, 'skip', which drops them, and 'keep', which is what we want. It will assign all unseen labels to a single category index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_fields = ['Pclass','SibSp','Parch']\n",
    "\n",
    "string_fields = [field.name for field in df3.schema.fields if field.dataType == types.StringType()]\n",
    "\n",
    "continuous_fields = ['Age','Fare']\n",
    "\n",
    "target_field = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex', 'Embarked']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# handleInvalid: If there is no value, a new category is added for these values\n",
    "string_indexers =[StringIndexer(inputCol=field,outputCol=field+'Index',handleInvalid='keep') for field in string_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OneHotEncoder\n",
    "\n",
    "A [OneHotEncoder](https://spark.apache.org/docs/2.2.0/ml-features.html#onehotencoder) generates a n-1 length vector column for an n-category column of category indices. \n",
    "\n",
    "We need to specify an input and an output column.\n",
    "\n",
    "One OneHotEncoder per categorical column. We are also going to build these stages programatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoders_cat =[OneHotEncoder(inputCol=field,outputCol=field+'OneHot') for field in categorical_fields if field not in string_indexers] \n",
    "encoders_str =[OneHotEncoder(inputCol=field+'Index',outputCol=field+'OneHot') for field in string_fields] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PclassOneHot',\n",
       " 'SibSpOneHot',\n",
       " 'ParchOneHot',\n",
       " 'Age',\n",
       " 'Fare',\n",
       " 'SexOneHot',\n",
       " 'EmbarkedOneHot']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "cols_to_concatenate = [ field + 'OneHot' for field in categorical_fields] + continuous_fields + [field +'OneHot' for field in string_fields]\n",
    "cols_to_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=cols_to_concatenate, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline\n",
    "\n",
    "Now that we have all the stages, we are finally ready to put them together into a single Estimator, our Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_83447b9094f5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages= string_indexers +\n",
    "                            encoders_str +\n",
    "                            encoders_cat +\n",
    "                            [assembler]\n",
    "                            )\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gone to the trouble of building our Pipeline, fitting it and using it to predict the probabilty of delay on unseen data is as easy as using a single Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_fit = pipeline.fit(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+-------------+-------------+--------------+-------------+-------------+-------------+--------------------+--------+\n",
      "| Age|   Fare|SexIndex|EmbarkedIndex|    SexOneHot|EmbarkedOneHot| PclassOneHot|  SibSpOneHot|  ParchOneHot|            features|Survived|\n",
      "+----+-------+--------+-------------+-------------+--------------+-------------+-------------+-------------+--------------------+--------+\n",
      "|22.0|   7.25|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[1],[1.0])|(6,[0],[1.0])|(21,[4,8,14,15,16...|       0|\n",
      "|38.0|71.2833|     1.0|          1.0|(2,[1],[1.0])| (3,[1],[1.0])|(3,[1],[1.0])|(5,[1],[1.0])|(6,[0],[1.0])|(21,[1,4,8,14,15,...|       1|\n",
      "|26.0|  7.925|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[3,8,14,15,17...|       1|\n",
      "|35.0|   53.1|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|(3,[1],[1.0])|(5,[1],[1.0])|(6,[0],[1.0])|(21,[1,4,8,14,15,...|       1|\n",
      "|35.0|   8.05|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[3,8,14,15,16...|       0|\n",
      "|54.0|51.8625|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|(3,[1],[1.0])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[1,3,8,14,15,...|       0|\n",
      "| 2.0| 21.075|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[3],[1.0])|(6,[1],[1.0])|(21,[6,9,14,15,16...|       0|\n",
      "|27.0|11.1333|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[0],[1.0])|(6,[2],[1.0])|(21,[3,10,14,15,1...|       1|\n",
      "|14.0|30.0708|     1.0|          1.0|(2,[1],[1.0])| (3,[1],[1.0])|(3,[2],[1.0])|(5,[1],[1.0])|(6,[0],[1.0])|(21,[2,4,8,14,15,...|       1|\n",
      "| 4.0|   16.7|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[1],[1.0])|(6,[1],[1.0])|(21,[4,9,14,15,17...|       1|\n",
      "|58.0|  26.55|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|(3,[1],[1.0])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[1,3,8,14,15,...|       1|\n",
      "|20.0|   8.05|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[3,8,14,15,16...|       0|\n",
      "|39.0| 31.275|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[1],[1.0])|(6,[5],[1.0])|(21,[4,13,14,15,1...|       0|\n",
      "|14.0| 7.8542|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[3,8,14,15,17...|       0|\n",
      "|55.0|   16.0|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|(3,[2],[1.0])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[2,3,8,14,15,...|       1|\n",
      "| 2.0| 29.125|     0.0|          2.0|(2,[0],[1.0])| (3,[2],[1.0])|    (3,[],[])|(5,[4],[1.0])|(6,[1],[1.0])|(21,[7,9,14,15,16...|       0|\n",
      "|31.0|   18.0|     1.0|          0.0|(2,[1],[1.0])| (3,[0],[1.0])|    (3,[],[])|(5,[1],[1.0])|(6,[0],[1.0])|(21,[4,8,14,15,17...|       0|\n",
      "|35.0|   26.0|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|(3,[2],[1.0])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[2,3,8,14,15,...|       0|\n",
      "|34.0|   13.0|     0.0|          0.0|(2,[0],[1.0])| (3,[0],[1.0])|(3,[2],[1.0])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[2,3,8,14,15,...|       1|\n",
      "|15.0| 8.0292|     1.0|          2.0|(2,[1],[1.0])| (3,[2],[1.0])|    (3,[],[])|(5,[0],[1.0])|(6,[0],[1.0])|(21,[3,8,14,15,17...|       1|\n",
      "+----+-------+--------+-------------+-------------+--------------+-------------+-------------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_transform = df3_fit.transform(df3)\n",
    "df3_transform.select('Age',\n",
    "                     'Fare',\n",
    "                     'SexIndex',\n",
    "                     'EmbarkedIndex',\n",
    "                     'SexOneHot',\n",
    "                     'EmbarkedOneHot',\n",
    "                     'PclassOneHot',\n",
    "                     'SibSpOneHot',\n",
    "                     'ParchOneHot',\n",
    "                     'features',\n",
    "                     'Survived').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData) = df3_transform.randomSplit([0.8, 0.2],seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(577, 135)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RandomForestClassifier\n",
    "\n",
    "And we are ready to do some Machine Learning! We'll use a RandomForestClassifier to try to predict delayed versus non delayed flights, a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|(21,[1,4,8,14,15,...|\n",
      "|       1.0|       0|(21,[1,3,9,14,15,...|\n",
      "|       1.0|       0|(21,[1,3,10,14,15...|\n",
      "|       0.0|       0|(21,[1,3,8,14,15,...|\n",
      "|       0.0|       0|(21,[1,3,9,14,15,...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training algo\n",
    "rfModel = rf.fit(trainData)\n",
    "rf_prediction = rfModel.transform(testData)\n",
    "rf_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|   54|\n",
      "|       0|   81|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.groupby('Survived').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Evaluating RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForest is = 0.807407\n",
      "Test Error of RandomForest = 0.192593 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "print(\"Accuracy of RandomForest is = %g\"% (rf_accuracy))\n",
    "print(\"Test Error of RandomForest = %g \" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
